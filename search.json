[{"title":"法律模型微调实践参考","path":"/2024/04/24/法律模型微调实践/","content":"法律模型微调实践法律领域数据特点 法律领域的数据特点是语言表达精确标准化，法律语言需要准确无歧义地表达法律关系、权利义务,不能随意更改词汇。 目前大模型目前还是自回归形式，本质基于概率统计模型 思考在法律领域生成词汇如何更加specific，生成回答的时候对某一些词汇进行限定，conditional generation 法律领域有大量专业术语,如“诉讼时效”、“假处分”等语料结构化程度高 法律文书有严格的格式要求语料结构化程度高。 预训练或者微调的时候如何结合结构化的信息模板，将结构化的信息建模到与训练语言模型中 关联信息量大，一个法律案件往往会涉及多条法律法规、判例,需要综合分析 法律领域数据挑战语料稀缺相比通用语料法律领域语料规模较小,获取不易标注质量参差不齐法律领域标注工作既专业又耗时,标注质量不一语义理解要求高需要正确理解并运用专业术语,才能解决实际法律问题 通用大模型通用域LLM的局限缺乏领域知识无法解决实际法律领域LLM的挑战需要传达准确含义,理解法律术语,分析实际案例口 说到底还是领域知识的学习 Lawyer LLaMA总体思路和结构 法律知识注入–收集法律文本继续训练 领域技能学习–收集司法考试、法律咨询问题进行监督微调 训练过程 增强中文理解能力 注入法律领域知识 学习解决实际法律问题的能力 增强回复生成的可靠性 SFT 有监督训练 S3 在司法考试、咨询语料上与训练 S5 FINE TUNE 同时将S1的语料重新replay 指令微调Tuning Fine-tuning:先在大规模语料上进行预训练，然后再在某个下游任务上进行微调，如BERT、T5; Prompt-tuning:先选择某个通用的大规模预训练模型，然后为具体的任务生成一个prompt模板以适应大模型进行微调，如GPT-3; Instruction-tuning:仍然在预训练语言模型的基础上，先在多个已知任务上进行微调(通过自然语言的形式)，然后再推理某个新任务上进行zero-shot。 Lora微调 vs全量微调Lora微调 英文全称Low-Rank Adaptation of Large Language Models，直译为大语言模型的低阶适应，是一种PEFT (参数高效性微调方法)需要更少的GPU，但是效果比不上后者 全量微调 对整个模型都进行微调,需要更大的GPU,但是效果更好 代码 https://github.com/pengwei-iie/Lexi-lawyer https://huggingface.co/THUDM/chatglm-6b 数据集： 通用数据集： https://github.com/LianjiaTech/BELLE/tree/main/data/1.5M 专用数据集： https://github.com/LiuHC0428/LAW-GPT","tags":["LLMs"]},{"title":"倦怠社会","path":"/2024/04/19/倦怠社会/","content":"Author: 韩炳哲Summary: 现代功绩社会主体心理的映照Type: Article 开篇，作者指出神话故事里的普罗米修斯，就是倦怠社会里自我剥削式主体最初的隐喻。 古希腊神话中，普罗米修斯因为帮人类盗取火种，而被罚绑在山崖。宙斯每天派秃鹰白天啄食他肝脏，夜晚他又重新长出肝脏，这样普罗米修斯就承受着一种日复一日永无止境的折磨，被一种类似倦怠感的痛苦控制。 作者认为这就是现代功绩社会主体心理的映照：自身对自身施加暴力。 《倦怠社会》里所言“暴力不仅源于否定性，也源于肯定性；不仅来自他者或外来者，还来自同类。”这种自我施暴的剥削十分隐蔽。就像没人会责怪抖音的推送太过有趣，只会反思自己没有自制能力。我们可以反抗他人的压迫，却没法抵挡自我的“剥削”；我们以为自己可以在功成名就后退隐山林，实际上却早已在资本的裹挟下面目全非。 作者认为，过去人们处于传统的规训社会。在规训社会里，社会存在种种规定和禁令，给人们的自由带来很多束缚和限制。但也因为有规定的存在，社会剥削是存在边界的。 “21世纪的社会不再是一个规训社会，而是功绩社会。其中的成员也不再是驯化的主体，而是功绩主体。” 随着社会发展，人们进入了功绩社会。在功绩社会里，人们不再受外在的强权机构控制，也没有外力强迫剥削个人。 “但是摆脱了强权机构控制后，人们并没有导向自由，因为自由和约束几乎在同一时刻降临。”——原文摘抄 功绩社会里，人们投身于一种自我约束中，以达到“功绩”的最大化。当追求“功绩”日益严重时，自我约束发展成为了一种自我剥削。 “因为自我剥削伴随着一种自由的感觉。功绩主体不断剥削自我，直至精力枯竭。他发展出一种自我攻击，并往往以自我毁灭为终结。”——原文摘抄 外在的约束尚有边界，而这种自我的约束是360度全方位、无尽无边的。在功绩社会，自我剥削比他者剥削更有效率。功绩社会的一代人，终日伴随着疲劳、抑郁、不可摆脱的倦怠感。 简而言之，社会从资本家强迫劳动者、剥削劳动者的社会，变成了我们自愿自动压迫和剥削自己的社会。 凯恩斯预测大家有一天都会达到小康状态，从而感到满足，因为新增的收入会越来越没有意义，大家就不再追求更多的东西。然而现状，大家都看到了：宁愿卷死自己，也要累死他人才是常态。 功绩社会，大家都努力，你不努力就容易出现自我否定。但在不停的努力中，必然会出现倦怠。 超过一定经济水平以后，人们大部分的金钱，都用于购买从绝对意义上并不真正需要的东西，但拥有这些东西意味着快乐、优越感、自我感觉良好。用一个网红的词语大概是：炫耀性消费。 但普通大众是无法克服人性的，每个人都是在跟别人的比较中，定位自身，形成自我身份认证的。 作者韩炳哲认为现代互联网科技发展，以“点赞文化”为代表加速了功绩社会的发展。 互联网科技带来了过度的刺激、信息和资讯，从根本上改变了人们注意力的结构和运作方式。 人们的感知因为过多的信息变得分散，反过来影响时间管理方式——习惯多工作业。但这种多工作业方式并不是人类新掌握的技能，代表文明进步，确切说反而是一种倒退。这种过度刺激，会带来过度的“积极性”，刺激人们更多地去自我剥削。 随着生产力发展，自我剥削日益加剧，让工作和生活失去了目的和意义，生成难以克服的倦怠感。 “提高或者降低工作速度，并不能解决当下的危机，我们需要一种全新的生活心态，把我们从飞转的停滞状态中解救出来。”——原文摘抄 现代社会是一种强制的自由，即我们处于拥有自由的假象和幻影中。书本里称21世纪的社会是功绩社会，而我们则是功绩主体，相比于规训社会里的规训主体，功绩主体无须屈从任何人，或者说只屈从于自身，但尽管摆脱了统治机构，却没有导向自由，自由和约束几乎在同一时刻降临。 “一个人同时是囚犯和看守，受害者和施暴者”这句话是“强制自由”最好的印证。过量的正反馈会导致既定目标的偏离，过度的活跃也会转变为一种过度的消极，我们失去了否定、消极、愤怒以及沉思的能力，“生命在现代变成了生存”。 在我们这个倦怠的社会里面有了一个新的问题，我们绝不允许自己有一丝的无聊。 过度的这种社会的积极性呈现给我们的是一种过度的一种刺激，每天都在刺激我们，大量的信息和资讯。因为我们肯定了自己的这种无限可能了，所以我们需要不断的拓展自己，我们每天开始疯狂的看资讯学习，要突破自己。 于是有个电影台词说的，现代人再也不会留意日落了。 这个留意日落的状态其实它本身就是一种我们理想的一种无聊。 韩敏哲在这里认为，我们涣散的这种注意力，其实反而他并不会产生新的事物了。他只会重复或加速已经存在的事物，我们看似获得了无数的讯息和知识，疯狂的在运转自己。但其实我们把面铺的越广就越不会深入他。你害怕无聊，当5分钟过去了，你感到无聊了，就会马上寻找新的一个替代品。于是短文字短视频就成为我们的这个时代的新欢，他和过去的一生只能做一件事形成了两个极端。 而只有深度的这种注意力才会创造新的文化，这种深度的注意力恰恰来自于一种深度的无聊。 也许到了功绩社会的晚期，人们普遍就会觉醒了。我们下一次的思想革命的对象可能就是推翻剥削自我的那个自我。我们从疲劳的倦怠走向一个亲近世界的倦怠，疲劳的倦怠，使人相互的对立。而亲近世界的倦怠是我们坐在傍晚的湖边从工具的人中走出来，成为自然的人，自我松动，不再向内剥削了，而是向着世界延展。人的心灵回到了群居的时代。","tags":["Read"]},{"title":"LongLLMLingua","path":"/2024/03/21/LongLLMLingua ACCELERATING AND ENHANCING LLMS/","content":"LongLLMLingua: ACCELERATING AND ENHANCING LLMS IN LONG CONTEXT SCENARIOS VIA PROMPT COMPRESSIONAuthor: Microsoft CorporationType: Academic JournalKeywords: 提示语压缩 摘要在长篇背景场景中，大型语言模型（LLMs）面临三个主要挑战：更高的计算&#x2F;财务成本、更长的延迟和较差的性能。一些研究表明，LLMs的性能取决于输入提示中关键信息（与问题相关）的密度和位置。受到这些发现的启发，我们提出了LongLLMLingua，用于对输入提示进行压缩，以改善LLMs对关键信息的感知，从而同时解决这三个挑战。我们在包括单&#x2F;多文档问答、少样本学习、摘要、合成任务和代码完成在内的广泛长篇背景场景中进行评估。实验结果表明，LongLLMLingua压缩的提示可以在更小的成本下实现更高的性能。整个系统的延迟也减少了。例如，在NaturalQuestions基准测试中，LongLLMLingua相对于原始提示可以获得高达17.1%的性能提升，输入到GPT-3.5-Turbo的标记数量减少了约4倍。它可以在LongBench和ZeroScrolls基准测试中分别节省28.5美元和27.4美元每1,000个样本的成本。此外，当以2倍至10倍的压缩率压缩约10,000个标记的提示时，LongLLMLingua可以将端到端的延迟加速1.4倍至3.8倍。 1 引言ChatGPT和其他大型语言模型（LLMs）已经彻底改变了面向用户的语言技术，并正在成为越来越多应用程序中的关键组件。精心设计提示对于在特定的下游任务中实现更好的性能是必要的。常用的技术，如上下文学习（ICL）（Dong等，2023）、检索增强生成（RAG）（Lewis等，2020）和Agent（Park等，2023），正在推动提示变得越来越长，甚至达到数千个标记。多文档问答、代码完成和文档摘要等场景也需要处理长篇背景。当LLMs用于长篇背景场景时，存在三个主要挑战：（1）运行这些模型或从提供LLM服务的公司调用API所需的更高计算和财务成本。这对于资源有限的个人或较小的组织来说可能是一个重要的障碍。（2）与LLMs相关的更长延迟，这可能导致生成响应或预测时出现延迟，特别是在用户期望快速而准确的实时场景中，这是一个问题。（3）由LLMs的扩展窗口大小（Xiong等，2023）和提示中与问题相关的关键信息的低密度以及位置不敏感引起的较差性能。图1a显示，LLMs在下游任务中的性能可能随着提示中噪声信息的增加而降低（Shi等，2023）。此外，图1b中的紫色曲线表明，LLMs捕捉相关信息的能力取决于它们在提示中的位置（Liu等，2023）：当相关信息出现在输入上下文的开头或结尾时，它们实现最佳性能，并且如果相关信息位于长篇背景的中间，则性能会显著下降。 图1：(a) 在提示中噪声信息增加时，LLMs在下游任务中的性能可能降低。在这种情况下，我们基于地面实况或LongLLMLingua rk保留k个最相关的文档&#x2F;段落。较大的k意味着在提示中引入更多噪声。为了提高提示中关键信息的密度，我们提出了问题感知的粗到细的压缩方法。(b) LLMS捕捉相关信息的能力取决于它们在提示中的位置。为了减少中间信息的丢失，我们引入了一个文档重新排序机制。 受到这些观察的启发，我们提出了LongLLMLingua来解决这三个挑战。具体而言，我们使用高级而高效的LLMLingua（Jiang等，2023a）作为我们的提示压缩的基础框架，以解决前两个挑战，即降低成本和延迟。然而，在长篇背景的情况下，提示中与问题相关的关键信息的分布通常是稀疏的。现有的提示压缩方法，如LLMLingua（Jiang等，2023a）和Selective-Context（Li，2023），在压缩过程中不考虑问题的内容，可能在压缩结果中保留太多噪声信息，导致性能较差。在本文中，LongLLMLingua旨在增强LLM对提示中与问题相关的关键信息的感知，以解决长篇背景场景中的第三个挑战，即性能较差。图1b是一个例子。LongLLMLingua的基本原理是小型语言模型本质上能够捕捉与给定问题相关的关键信息的分布。我们的主要贡献有五个方面：(1) 我们提出了一个问题感知的粗到细的压缩方法，以提高提示中关键信息的密度（第4.1节）；(2) 我们引入了一个文档重新排序机制，以减少中间信息的丢失（第4.2节）；(3) 我们提出了动态压缩比率，以连接粗粒度压缩和细粒度压缩，进行自适应颗粒控制（第4.3节）；(4) 我们提出了一个后压缩子序列恢复策略，以提高关键信息的完整性（第4.4节）；(5) 我们在三个基准测试上评估了LongLLMLingua，即NaturalQuestions（Liu等，2023）、LongBench（Bai等，2023）和ZeroSCROLLS（Shaham等，2023）。实验结果表明，与原始提示相比，LongLLMLingua压缩的提示在更低的成本下可以实现更高的性能。整个系统的延迟也减少了。 2 问题阐述按照LLMLingua（Jiang等，2023a）的做法，我们使用x &#x3D; (xins, xdoc 1 , · · · , xdoc K , xque)来表示提示，其中包括指令xins，K个文档xdoc k 和问题xque。实际上，根据具体的应用场景，提示可以进行修改。例如，可以删除开头的xins，xque可以是用户指定的任何需求，而(xdoc 1 , · · · , xdoc K )可以是用户追加到提示中以获得LLMs对xque更好响应的任何附加材料。提示压缩系统的目标可以表示为： $ex​min​D(y,ey​)+λ∥ex​∥0​$ 其中e x表示压缩的提示，是x的标记级子序列。y代表具有x作为输入的地面实况输出文本，e y代表由LLM生成的结果。通过e x。D是两个分布之间的距离度量，例如KL散度。我们期望y和e y的分布尽可能相似。λ是有关压缩比的折衷超参数。在这项工作中，我们还将K个文档（xdoc 1 ，···，xdoc K ）的排列操作空间纳入联合优化考虑。 3 预备知识：LLMLINGUALLMLingua（Jiang等，2023a）使用一个小型语言模型MS计算原始提示中每个标记的困惑度，然后删除困惑度较低的标记。这种方法背后的原理是，困惑度较低的标记对语言模型整体熵增益的贡献较小，因此删除它们对LLM对上下文的理解影响相对较小。LLMLiungua由三个组件组成：一个预算控制器、一个迭代的标记级提示压缩算法和一个分布对齐机制，如图2中的斜体文本所示。预算控制器为原始提示的各个组件（即指令、演示、问题）分配不同的压缩比，并在演示级别进行粗粒度压缩。中间结果被划分为段，然后按段执行标记级压缩，每个标记的困惑度取决于MS计算的先前压缩段。对于分布对齐，它通过使用由目标LLM生成的数据对MS进行指令调整，缩小LLM分布与用于提示压缩的MS分布之间的差距。 4 LONGLLMLINGUALongLLMLingua是在LLMLingua框架的基础上发展起来的，用于在长篇背景场景中进行提示压缩。在长篇背景场景中，主要挑战是如何增强LLM对提示中与问题相关的关键信息的感知。LongLLMLingua从三个角度解决了这一挑战，并进一步应用子序列恢复策略来提高向用户提供的信息的准确性和可靠性。我们在本节详细阐述每个组件。 4.1 如何提高提示中的关键信息密度？问题感知的粗粒度压缩 在粗粒度压缩中，我们的目标是找到一个度量 rk 来评估每个文档 xdoc k &#x3D; {xdoc k,i }Nk i&#x3D;1 的重要性，其中 Nk 是 xdoc k 中的标记数量。我们只保留具有较高 rk 的 xdoc k 作为中间压缩结果。LLMLingua使用文档级困惑度来表示文档的重要性： $rk &#x3D; \\frac{1}{Nk} \\sum_{i&#x3D;1}^{Nk} p(xdoc_{k,i}) \\log p(xdoc_{k,i}), \\quad k \\in {1, 2, \\ldots, K}$ 尽管保留的文档通常包含大量信息，但它们与问题 xque 无关，反而成为噪声，降低了压缩结果中的关键信息密度，并给LLM正确输出答案带来困难。如图3a所示，LLMLingua的recall@16仅达到50%，表明其在压缩过程中无法保留关键信息。 在这里，基于检索的方法也是可行的。我们可以使用 xque 从 (xdoc 1 ，···，xdoc K ) 中检索出最相关的文档作为压缩结果。然而，这些方法难以区分与问题相关的细粒度语义信息。在检索过程中可能会放弃一些包含关键信息的文档。如图3a所示，基于嵌入的方法，如Sentence BERT和OpenAI Embedding，在recall@5上仅达到∼75%的准确率，这意味着LLMs进行4倍压缩的最终准确率上限仅为75%。 改善压缩结果中关键信息密度的一种方法是计算在问题 xque 的条件下文档级困惑度。然而，这种方法可能不够有效，因为文档通常包含大量无关信息。即使在条件为 xque 的情况下，对整个文档计算的困惑度分数可能不足够明显，使其成为文档级压缩的不足够指标。 因此，我们建议使用在不同上下文 xdoc k 条件下计算的问题 xque 的困惑度来表示它们之间的关联。我们在 xque 和 xrestrict 的连接序列之后追加了一个限制性语句 xrestrict2，以加强 xque 和 xdoc k 之间的相互关联。它可以被视为一种正则项，减轻了幻觉的影响。这可以表示为： $rk &#x3D; \\frac{1}{Nc} \\sum_{i} p(xque,restrict_i | xdoc_k) \\log p(xque,restrict_i | xdoc_k), \\quad k \\in {1, 2, \\ldots, K}$ 其中 xque,restrict_i 是连接序列 xque 和 xrestrict 的第 i 个标记，Nc 是标记的数量。图3a表明，我们的粗级别压缩方法在保留不同数量的文档时实现了最高的recall，表明它在压缩结果中保留了来自文档 (xdoc 1 ，···，xdoc K ) 的大多数关键信息。 问题感知的细粒度压缩 在细粒度压缩中，我们评估指令 xins、问题 xque 以及粗粒度压缩后保留的 K’ 个文档 {xdoc i }K’ i&#x3D;1 中每个标记的重要性。我们结合了LLMLingua后续的迭代压缩机制，并直接计算标记困惑度以压缩 xins 和 xque。在本节中，我们研究如何使对 {xdoc k }K’ k&#x3D;1 的细粒度标记级压缩感知到问题 xque，以便压缩结果能够包含更多与问题相关的关键信息。 对于对 xque 的感知的一个直接解决方案是简单地将其连接到整个上下文的开头。然而，这将导致在条件后续上下文的相关标记的困惑度降低，进一步减少它们与一般标记的区分度。在本文中，我们提出对比困惑度，即由于问题条件引起的分布偏移，以表示标记与问题之间的关联。每个 {xdoc k }K′ k&#x3D;1 中的标记 xi 的基于对比困惑度的重要性度量 si 可以表示为： $si &#x3D; \\text{perplexity}(xi|x&lt;i) - \\text{perplexity}(xi|xque, x&lt;i)$ 图3b说明了困惑度和对比困惑度之间的差异。我们可以看到，困惑度较高的标记广泛分布在所有文档中。然而，对比困惑度较高的标记更集中在虚线左侧，对应包含问题答案的文档。这表明所提出的对比困惑度可以更好地区分与问题相关的标记，从而提高压缩结果中的关键信息密度。 4.2 如何减少中间信息损失？如图1b所示，在长篇背景中，当相关信息出现在开头时，LLM的性能最好，如果相关信息位于中间位置，则性能显著下降。在粗粒度压缩之后，我们已经获得了一组文档 {xdoc k }K′ k&#x3D;1 以及它们的相关性分数 {rk}K′ k&#x3D;1，指示它们与问题 xque 的关联。因此，我们使用它们的相关性分数重新排序文档，以更好地利用LLMs在位置上的信息感知差异： $(xins, xdoc_1, \\ldots, xdoc_{K’}, xque) \\xrightarrow{rk} (xins, xdoc_{r1}, \\ldots, xdoc_{rK’}, xque)$ 4.3 如何在压缩过程中实现自适应颗粒控制？在细粒度压缩中，LLMLingua对从粗粒度压缩中获得的所有文档应用相同的压缩比。然而，不同文档的关键信息密度是不同的。文档与问题相关性越高，我们应该分配给它的预算（即较低的压缩比）就越多。因此，我们将粗粒度压缩获得的相关性分数 {rk}K′ k&#x3D;1 用于指导细粒度压缩中的预算分配，以实现整体的自适应颗粒控制。 具体而言，我们首先使用LLMLingua的预算控制器确定保留文档的初始预算 τ doc 3。在细粒度压缩过程中，我们按照LLMLingua中的迭代标记级压缩算法，但根据粗粒度压缩的重要性分数的排名索引 I(rk)（例如，0、1）动态分配压缩预算 τ doc j 给每个文档 xdoc k。在这篇论文中，我们采用线性调度程序进行自适应分配。每个标记 xi 的预算可以表示为： $\\tau_i &#x3D; \\tau doc_k, \\quad \\tau doc_k &#x3D; \\max\\left(\\min\\left((1 - 2I(rk) N_d)\\delta\\tau + \\tau doc, 0\\right), 1\\right)$ 其中 N_d 表示文档的数量，δτ 是一个控制总体分配预算的超参数。 4.4 如何提高关键信息的完整性？在细粒度标记级的压缩过程中，可能会放弃一些关键实体的标记。例如，原始提示中的时间实体“2009”可能被压缩为“209”，名字实体“Wilhelm Conrad Röntgen”可能被压缩为“Wilhelmgen”。这可能会导致基于事实的任务（例如文档问答）出现问题，其中语言模型倾向于从提示中复制信息，如图4所示，为了提高向用户提供的信息的准确性和可靠性，我们提出了一个子序列恢复方法，以从LLMs的响应中恢复原始内容。该方法依赖于原始提示、压缩提示和LLMs响应中的标记之间的子序列关系。整体流程包括：i）迭代LLMs响应中的标记 yl，并选择在压缩提示 e x 中出现的最长子字符串 e ykey,l &#x3D; {yl, yl+1, …, yr}。ii）在原始提示 x 中找到与原始提示中的表示 e ykey,l 对应的最大公共最短子序列 xi,j &#x3D; {xi, xi+1, …, xj}（可加速使用前缀树或序列自动机）。iii）用原始提示中相应的子序列 xi,j 替换LLMs响应中匹配的标记 e ykey,l。更多详细信息，请参阅Algorithm 1。 5 实验证明在这里，我们研究：(1) LongLLMLingua有多有效？(2) LongLLMLingua有多高效？ 实施细节 在本文中，我们使用GPT-3.5-Turbo-06134和LongChat-13B-16k作为目标LLMs，均可通过OpenAI5和HuggingFace6访问。为确保稳定且可重复的结果，在所有实验中我们采用贪婪解码，并将温度设置为0。对于用于压缩的小语言模型，我们应用经过监督微调和RLHF对齐的LLaMA-2-7B-Chat7。我们使用PyTorch 1.13.1和HuggingFace Transformers实现我们的方法。我们设置超参数，遵循LLMLingua，除了在这里将用于迭代标记级压缩的段大小设置为200。附录B提供了更多详细信息。 数据集和评估指标 我们在多文档QA任务中使用NaturalQuestions，并在一般长篇背景情境中使用LongBench和ZeroSCROLLS。 (i) NaturalQuestions (Liu et al., 2023): 该基准类似于商业搜索和问答场景（如Bing Chat）中的检索增强生成设置。具体而言，每个问题在原始提示中有20个相关文档。其中一个包含正确答案，而在提示中有五个不同的地面真实文档位置设置：第1、5、10、15和20个。与Liu et al. (2023)一样，我们使用准确性作为评估指标。(ii) LongBench (Bai et al., 2023): 该基准包括六种任务类型：单文档QA、多文档QA、摘要、少量样本学习、代码完成和合成任务。我们使用涵盖16个数据集的英语部分进行评估。我们使用基准提供的度量和脚本进行评估。(iii) ZeroSCROLLS (Shaham et al., 2023): 该基准包括四种任务类型：摘要、QA、情感分类和重新排序，涵盖了10个数据集。我们使用验证集进行评估。我们使用提供的度量和脚本进行评估。 基线 在以下实验中，我们包括两组基线：(i) 基于检索的方法。我们使用五种SoTA检索方法衡量问题和提示中文档之间的关联：BM25、Gzip (Jiang et al., 2023b)、SentenceBERT (Reimers &amp; Gurevych, 2019)、OpenAI Embedding，以及在LongLLMLingua粗粒度压缩。我们丢弃与低关联的句子或段落，直到满足压缩约束，同时保持原始文档顺序不变。(ii) 基于压缩的方法。我们将我们的方法与两种用于提示压缩的最新方法进行比较，即Selective Context（Li，2023）和LLMLingua（Jiang et al., 2023a）。这两种方法都使用LLaMA-2-7B-Chat作为用于压缩的小语言模型。在LLMLingua中，采用了一种从粗到细的方法来处理压缩比例的约束：首先，在粗略级别将原始提示压缩到约束的k倍，其中k是粒度控制系数；然后，在标记级别上执行以达到总体约束。我们的方法遵循相同的粗到细逻辑以实现约束。 主要结果 表1和表3展示了不同压缩约束下各种方法的性能。有多个观察和结论：(1) 我们的LongLLMLingua在不同任务和压缩比例约束下均取得了最佳性能。与原始提示相比，我们的压缩提示可以以更少的成本获得更高的性能。例如，在NaturalQuestions中，LongLLMLingua在地面真实文档位于第10位置的情况下提高了17.1%的性能，而输入到GPT3.5-Turbo的标记数∼少了4倍。(2) Selective Context（Li，2023）和LLMLingua（Jiang et al., 2023a）等基于压缩的方法在大多数任务上表现不佳，尤其是在原始提示中存在大量不相关信息的任务。这是由于它们纯粹基于信息熵的压缩机制，在压缩结果中包含太多噪音，甚至导致性能比零射击设置更差，例如在NaturalQuestions上。(3) 检索-based方法在低压缩率下表现良好。然而，随着压缩的进行，它们的性能下降，例如，2x→4x；3000个标记→2000个标记。这可能是由于召回减少引起的。图3a是NaturalQuestions上的案例说明。(4) LongLLMLingua以及我们的粗粒度压缩度量rk都比不同任务和压缩约束下的所有其他基线更加稳健。随着压缩率的增加，例如，2x→4x，LongLLMLingua甚至取得了一点性能提升。我们主要归功于问题感知的粗到细压缩，这可以更好地找出关键信息，并在更高的压缩率下实现更高的关键信息密度。(5) 所提出的文档重新排序策略在我们的方法中以及在表1中的其他基线中都是有效的，充分证明了它的有效性。 消融研究 为了评估LongLLMLingua中不同组件的贡献，我们引入了六个它的变体进行消融研究： 没有Question-aware Coarse-grained的我们，该变体使用LLMLingua中的信息熵计算问题文本关联rk。 使用SBERT的我们，该变体使用SBERT计算rk。 没有Question-aware Fine-grained的我们，该变体忽略了方程（3），仅应用LLMLingua的迭代标记级提示压缩。 没有Dynamic Compression Ratio的我们，该变体在精细压缩中所有文档共享相同的压缩比率。 没有Subsequence Recovery的我们，该变体去除了后处理子序列恢复策略。 使用Subsequence Recovery的LLMLingua，该变体添加了后处理子序列恢复策略。 表2展示了消融研究的结果。总体而言，去除LongLLMLingua中任何一个组件都会导致性能下降，无论地面真实答案的位置如何。这充分验证了在粗到细的压缩过程中所提出的问题感知机制、动态压缩比率和子序列恢复策略的必要性和有效性。同时，结果表明，在粗粒度压缩中使用SBERT会导致性能较差，这暗示了我们在方程（2）中的问题感知重要性度量优于SBERT。此外，我们的子序列恢复策略也能为LLMLingua带来性能提升。然而，没有我们的问题感知机制，LLMLingua的结果仍然令人不满意。详细案例请参见附录C。 延迟评估 我们在V100-32G GPU上进行测试，使用来自LongBench的提示，平均约为10K个标记，并在API调用中将响应长度设置为200个标记。在表5中，E2E表示来自提示压缩系统和黑盒API的总体延迟，而LongLLMLingua表示仅提示压缩的延迟。结果显示我们的提示压缩系统确实加速了整体推理。随着压缩率的增加，加速效果变得更加显著。值得一提的是，在API花费时间更长的场景中，LongLLMLingua实际上节省的绝对时间可能更为显著。 相关工作长上下文的LLMs。最近的研究集中在扩大LLMs的上下文窗口大小上。主要的方法包括：（1）分阶段的预训练（Nijkamp等人，2023），逐渐增加上下文窗口；（2）修改（Press等人，2022）或插值位置嵌入（Chen等人，2023；Peng等人，2023；Han等人，2023）；（3）使用线性或稀疏注意机制（Ding等人，2023；Sun等人，2023）；（4）利用外部存储上下文的记忆模块（Bertsch等人，2023；Tworkowski等人，2023）。尽管这些方法解决了上下文窗口的扩展，但它们对下游任务性能的影响尚未讨论。 提示中的信息分布。最近的经验实验证明，LLMs在提示中包含的信息较少时性能下降（Bai等人，2023；Li等人，2023；Shi等人，2023）。而且，提示中相关信息的位置对性能有显著影响（Wu等人，2022）。Liu等人（2023）指出，相比于提示两端的信息，LLMs更难理解位于提示中部的信息。 检索方法可以分为密集或稀疏检索方法。稀疏检索方法，如BM25，基于n-gram信息确定查询和文档之间的相关性。相反，密集检索方法使用密集向量在潜在空间中评估查询和文档之间的相关性，例如SentenceBERT（Reimers＆Gurevych，2019）和OpenAI Embedding。最近，Jiang等人（2023b）提出了一种无监督的密集检索方法，利用传统的压缩算法，如gzip和k最近邻。 提示压缩方法可以分为三类主要类别：（1）标记修剪（Goyal等人，2020；Kim＆Cho，2021；Modarressi等人，2022）和标记合并（Bolya等人，2023），需要在推理过程中进行模型微调或中间结果，并已用于BERT规模的模型。 （2）GIST（Mu等人，2023）、AutoCompressor（Chevalier等人，2023）和ICAE（Ge等人，2023）等软提示调整方法，需要对LLMs的参数进行微调，使它们适用于特定领域但不直接适用于黑盒LLMs。 （3）基于信息熵的方法，如Selective Context（Li，2023）和LLMLingua（Jiang等人，2023a），使用小型语言模型计算原始提示中每个标记的自信息或困惑度，然后删除困惑度较低的标记。 结论我们提出了LongLLMLingua来解决LLMs在长上下文场景中面临的三个挑战，即更高的计算&#x2F;财务成本、更长的系统延迟以及性能下降。我们从高效提示压缩的角度发展了LongLLMLingua，从而降低了计算&#x2F;财务成本和系统延迟。我们进一步设计了四个组件，即问题感知的粗到细的压缩方法、文档重新排序机制、动态压缩比率和后压缩子序列恢复策略，以提高LLMs对关键信息的感知。通过这些组件，LongLLMLingua展现了卓越的性能。在一个多文档QA基准和两个长上下文基准上的实验证明，LongLLMLingua压缩的提示可以在减少API推理的计算&#x2F;财务成本和端到端系统延迟的同时获得比原始提示更高的性能。 以下来自原作者的知乎内容News我们制作了一个Project Page，来展示现实场景中压缩的cases，包括 RAG、 Online Meeting、CoT 和Code； LongLLMLingua已经整合到了LlamaIndex Pipeline中，这是一个广泛使用的RAG框架。 Tl;DR: LLMLingua, 利用经过Alignment的well-trained的小的语言模型，例如GPT2-small或者LLaMA-7B，来检测和剔除prompt中的不重要token，将其转化为一种人类很难理解但是LLMs能很好理解的形势。并且这种被压缩的prompt可以直接用在black-box LLMs中，实现最高20倍的压缩，且几乎不影响下游任务性能，尤其是LLMs特有的能力，例如ICL，Reasoning等。 LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models (EMNLP 2023). Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang and Lili Qiu LongLLMLingua, 利用Prompt 压缩增强LLMs在Long Context Scenarios下感知prompt中关键信息的能力，能够有效缓解Lost in the Middle, 十分适合RAG场景中使用。实现每1k个样本节省最高$28.5(GPT-3.5-Turbo, 4的话这个值还能x10)，并且还能提升LLMs的性能。 LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression (Under Review). Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang and Lili Qiu 以下内容仅代表个人观点，不代表我所在公司或者团队的观点。 既然是博客，我们就换一种方式从high level角度讲一下我们对这个工作的理解。 首先，感谢 @九号 之前的分享，启发我们在这个topic上面做的一些工作。 标题以及LLMLingua的名字其实表达的是我们的一个goal，通过某种方式，构造一种专属于LLMs的语言。 这种语言很有可能人类很难理解，但是LLMs可以很好的理解，从而可以在这种语言上面做一些有趣的事情。 比如说用更少的比特数传递更多的信息，甚至可以传递给LLMs更高效的信息，从而提升LLMs的性能。 而我们选择了从Prompt压缩这条路来接近这个goal。 而且通过Prompt压缩来做这件事非常Make Sense，其一是因为Natrual Language本身是冗余的，尤其是口语下，第二是ChatGPT实际上能很好的理解某种被压缩的文字[1]. 如果从LLMs is Compressor 角度来想这个问题，会变得更顺理成章。即prompt中更高置信的部分token完全不需要交给LLMs，他也能从上下文中猜测出来。 这也就是我们这系列工作的一个核心出发点。 而压缩带来的Inference 加速，API Cost节省都是实现这个goal的一些副产物，只是这些副产物已经非常吸引人了。 而在LongLLMLingua这篇工作中，我们讨论了Long Context Scenoias 下prompt低信息密度的现象，并进一步利用了信息密度及位置与LLMs performance的关系，提出了一种Question-Aware 的方法来增强LLMs 感知关键信息的能力。 接下来我会分别介绍这两个工作。 LLMLingua我个人是非常喜欢这篇工作，我们在这篇中做了很多细致的分析，得到了一些很有趣的发现。 Paper里我们试图去回答以下几个问题，不过可能有些有趣的发现因为在较后的位置，甚至是附录，可能会被大家忽略。 我们应该如何去设计一个prompt 压缩算法，从而能够最大化的压缩prompt，同时又不影响LLMs的性能。 这种被压缩的prompt能直接用在下游任务中吗？ 它的泛化性怎么样？ 有什么证据能证明Black-box LLMs能理解这种被压缩的prompt？ 这种压缩方法的有上界吗？ 为什么不用GPT-X来做这件事？ 第一是如何去系统的设计一个Prompt Compression方法，我们的Goal是能直接用，不需要对LLMs额外的训练，而且能平衡语言完整性和压缩率之间的关系。 我们想到了OoD里面基于PPL的一系列方法，OoD的信息正好是Prompt给到LLMs的有效信息。 这条路会有别于Token Pruning&#x2F;Merge, 亦或是Soft prompt based method, 从而有希望在不对LLMs进行额外训练的情况下，直接将被压缩的prompt用在black-box LLMs上。 但仅仅基于ppl就够了么？我们发现 prompt中不同成分对于压缩的敏感程度是不同的，例如System prompt，question prompt的敏感度更高; 高压缩率下，过度影响语言完整性，会让LLMs难以理解prompt中的信息； 部分Context 中的信息是冗余重复的，尤其是ICL场景下; PPL 分布会随着压缩的发生而发生改变； 需要有某种方式让小模型aware Black-box LLMs； 这也就是我们在LLMLingua中设计了Budget Controller，Iterative Token-level Prompt Compression, Alignment 三个module的原因，具体细节可见paper。 第二个问题也是所有Efficient Method in LLMs都会遇到的问题，不过之前大部分工作也只是在一些传统的Zero-shot QA或者Language Model task上进行测试，为了进一步说明这种被压缩prompt对于下游任务的影响，我们专门从LLMs特有的一些能力出发，评测了ICL，Reasoning，Summarization，和Conversation这些任务。结果显示我们在GSM8K上可以做到20x的压缩比，并且几乎不影响performance。在Summarization和Conversatio的结果也比baseline要优。 顺带回答第六个问题，其实可以看见Generation-based的方法实际上不能很好的保留精心设计的prompt中的关键信息，它会忽略推理细节，甚至生成一个完全不相关的examples，即使是GPT-4 也很难完成压缩prompt这件事。 为了证明LLMLingua的泛化性，我们测试了不同small language model 和black-box LLMs，结果显示由于我们的设计GPT2 small size的模型也能取得不错的结果。此外，被压缩的prompt也能在Cluade上取得不错的结果，这也说明了LLMLingua的泛化性。 我们还做了一个有趣的实验，让GPT-4 去帮助回复被压缩之后的prompt，惊奇的发现，居然可以从那些人类很难理解的文本中几乎完全的恢复出所有细节，如下图完全恢复出了9-steps CoT。不过不同压缩程度的prompt能恢复的细节也不同，这也说明了我们的设计是合理的。 我们还发现压缩Prompt不仅能节省Input tokens，还能进一步节省20%-30% output tokens. 我们也尝试使用了更多的压缩率，结果显示即使是利用了LLMLingua，在特别大的压缩率下仍然会出现特别剧烈的performance drop。 除此之外，我们还将LLMLingua apply到KV-Cache Compression的场景，也能取得不错的performance。 LongLLMLinguaLongLLMLingua 出发点和LLMLingua就不太一样了，不只是想要压缩prompt保证少掉精度，而是想要在Long Context Scenarios下，通过提升prompt中的信息密度，从而提升LLMs的性能。 这个方法特别适合现在普遍使用的Retrieval-Augmented Generation Method。 虽然现在有很多工作让模型能够处理更长的context，但是Context Windows 的增长反而会影响很多下游任务的performance[2]; 其次，之前的工作表面prompt中noise的增多，会影响LLMs的性能; Lost in the middle 中分析了prompt 中关键信息的位置对于LLMs的性能的影响; Long Context Prompt 会带来更多的API Cost和latency; 综合以上几点，我们觉得Long Context Scenorias 中信息密度是一个非常关键的问题，Prompt Compression可能是其中的一个解决方案。 但是LLMLingua或者其他Compression-based 的method 并不是一个合适的解决方案，原因是Long Context 中关键信息的密度很低，很有可能prompt本身的信息熵很高，但是不相关，这样压缩prompt反而会引入更多的噪声，从而影响performance。 我们的解决方案是，通过设计了Question-Aware Coarse-fine的方法，让压缩算法能够感知到因为question带来的关键信息分布的变化。 具体 可以看到如上右图，在4x压缩率，我们的方法能够略微超过ground truth 位于prompt 开头的结果，从而用更少的API Cost取得更好的结果，缓解lost in the middle 带来的问题。 我们还设计了dynamic compression ratio 来串联两个粒度方法的信息，设计了一个基于子序列的后处理recovery来恢复被压缩的重要信息。 为了证明我们方法的有效性，我们在Multi-Document QA和两个Long Context Benchmark 中进行了细致的测试。 其中Multi-Document QA中选用的dataset 更贴合RAG实际场景，k个document均为粗排召回与question十分相关的document。 可以看到我们的方法通过提高prompt中的信息密度能够有效的提升performance，缓解lost in the middle现象，尤其是经过reorder之后。 其次，即使单独使用LongLLMLingua中Coarse-level Question-aware的方法，也能取得不错的效果，这也说明了我们的设计有效性。 我们还测试了Long Context Benchmark中的不同tasks，包括Single-Document, Multi-Document, Summarization, Few-shot Learning, Synthetic和Code补全。 结果显示我们的方法在Multi-Document QA， Synthetic等任务上提升明显，能够在6x压缩率下获得更好的performance。 除此之外，我们还测试了端到端Latency，和API Cost节省情况。 结果显示，LongLLMLingua 虽然会比LLMLingua慢，但仍然能拿到实际的端到端加速。 API Cost方面，Long Context Scenorias 下能够节省更多的Cost，最多每1k个样本节省$28.5。 FQA 不同LLMs之间Tokenizer 不同会影响压缩效果吗？ 实际上我们用的小模型和大模型之间的Tokenizer完全不同，但是并不影响大模型对compressed prompt的理解。Token-level Compression是为了进一步感知LLMs中对于prompt的token的重要分布。 你看过那么多压缩的example，你觉得人能总结出某种语法吗？ 我觉得十分困难，Natural Language本身有一定的语法规则，但是LLMLingua所蕴含的是语法+world Knowledge，这种Knowledge是很难由某些特定人完全掌握的。 为什么LLMs能理解Compressed Prompt？ 我们现在的理解是因为world Knowledge是相同的，不同的LLMs其实都是对于同一个Knowledge Space的逼近，不同LLMs能够逼近的程度不同（可以看成是LLMs的压缩率）。 传统压缩算法能做Prompt Compression吗？ 实际上我们觉得直接做Prompt Compression比较困难，因为压缩完的prompt很有可能token数量不会减少，而且LLMs并不会很好的理解这种格式的信息。但是可以将传统压缩算法看做是某种Retrieval-based Method 来做Coarse-level的prompt compression。 能利用不同语言信息熵不同的特点，将prompt转换为信息熵高的语言再压缩吗？ 理论上是可以的，不同语言中的信息熵差异非常大。但是这取决于translation System能够保留住原有信息，且black-box LLMs对于对应language能够有与源语言相似的performance。 在完成LLMLingua之后，由于*CL的匿名政策，让我们有了充足的时间去思考。在这里感谢公司内部的一些同事，对我们的工作提出了很多有意义的问题，这些问题也帮助我们更好的理解我们的工作，以及未来的方向。","tags":["LLMs"]},{"title":"SCOTT_ Self-Consistent Chain-of-Thought Distillation","path":"/2023/10/29/SCOTT Self-Consistent Chain-of-Thought Distillation/","content":"SCOTT: Self-Consistent Chain-of-Thought DistillationAuthor: Amazon, University of Southern CaliforniaSummary: 大型语言模型（LMs）在一定规模以上展现了通过思维链（CoT）提示生成自由文本理由的新兴能力。虽然CoT可以带来显著的性能提升，但这种收益只在足够大的LMs中观察到。更令人担忧的是，生成的理由是否与LM的预测一致，或者是否忠实地证明了决策，很难保证。在这项工作中，我们提出了SCOTT，一种忠实的知识蒸馏方法，从一个数量级更大的教师模型中学习一个小型、自洽的CoT模型。为了形成更好的监督，我们通过对比解码从一个大型LM（教师）中引出支持黄金答案的理由，这鼓励教师生成的令人信服的令牌只有在考虑答案时才变得更加合理。为了确保忠实的蒸馏，我们使用教师生成的理由来学习一个具有反事实推理目标的学生LM，这样可以防止学生忽视理由而做出不一致的预测。实验证明，尽管产生了可比较的最终任务性能，我们的方法可以生成比基线更忠实的CoT理由。进一步的分析表明，这样的模型在做决策时更尊重理由；因此，我们可以通过改进其理由来提高其性能。Type: Academic JournalPublisher: ACL2023Score &#x2F;5: ⭐️⭐️⭐️⭐️⭐️Keywords: 忠实的知识蒸馏，反事实推理目标的学生LM 摘要大型语言模型（LMs）在一定规模以上展现了通过思维链（CoT）提示生成自由文本理由的新兴能力。虽然CoT可以带来显著的性能提升，但这种收益只在足够大的LMs中观察到。更令人担忧的是，生成的理由是否与LM的预测一致，或者是否忠实地证明了决策，很难保证。在这项工作中，我们提出了SCOTT，一种忠实的知识蒸馏方法，从一个数量级更大的教师模型中学习一个小型、自洽的CoT模型。为了形成更好的监督，我们通过对比解码从一个大型LM（教师）中引出支持黄金答案的理由，这鼓励教师生成的令人信服的令牌只有在考虑答案时才变得更加合理。为了确保忠实的蒸馏，我们使用教师生成的理由来学习一个具有反事实推理目标的学生LM，这样可以防止学生忽视理由而做出不一致的预测。实验证明，尽管产生了可比较的最终任务性能，我们的方法可以生成比基线更忠实的CoT理由。进一步的分析表明，这样的模型在做决策时更尊重理由；因此，我们可以通过改进其理由来提高其性能。 引言大型语言模型（LMs）通过思维链（CoT）提示引出强大的推理能力（Wei等，2022b），其中要求LMs生成自由文本的理由来解释其多步推理。然而，CoT提示并不保证理由与预测一致，使得理由无法用于证明模型的行为。在本文中，我们提出了一种名为Self-Consistent Chain-Of-Thought DisTillation（SCOTT）的知识蒸馏（KD）方法，用于引出忠实的CoT推理，即小型学生模型从大型教师模型中学习，生成与其预测一致的CoT理由。 现有的研究（Shridhar等，2022；Li等，2022a）主要提出从大型LMs中学习推理，主要是为了计算效率或任务性能。它们提示一个大型的LM（教师）为下游数据集生成理由，然后用这些理由来训练一个小型的LM（学生）。然而，这些研究忽视了可能削弱理由的忠实度的两个问题。首先，LMs容易产生幻觉，意味着它们经常生成与输入不相关的文本（Maynez等，2020；Ji等，2022）因此，教师可能不会生成与答案完全支持的主题相关的理由。在我们的先导性研究中（图1），通过GPT-3生成的100个随机理由中，我们发现其中42%的理由没有提供在任务输入中没有提到的新信息，37%的理由没有对答案进行合理的解释。这种理由与答案之间的不一致将会传给学生。其次，学生可能将理由生成和答案预测视为两个独立的过程。这是由于问题和答案之间的虚假相关性，学生利用这种推理快捷方式（Branco等，2021）。这两个问题加在一起将导致一个不忠实的学生，学会生成空洞的理由，并可能做出与理由不一致的预测。 为了解决这些问题，我们提出了从两个方面分别增强基本的知识蒸馏过程。 为了从教师那里引出更多与主题相关的理由，我们提出利用对比解码的方法，旨在将每个理由与答案联系起来（§ 3.1）。这种技术鼓励教师在解码过程中生成更合理的令token，只有在考虑答案时才更合理，而不是在没有答案的情况下也相对合理。 为了训练一个忠实的学生，我们要求学生进行反事实推理，即当理由导致不同的答案时进行相应的预测（§ 3.2）。我们通过要求教师为抽样的错误答案生成一个理由来获得训练数据。由于现在学生需要根据训练过程中提供的理由给出相同问题的不同答案，因此问题和正确答案之间的推理捷径被消除了。 我们在几个需要知识密集型推理的开放领域问答任务上进行了实验。实验结果表明：（1）对比解码可以导致更一致的教师，生成更支持正确答案的理由。（2）在更一致的理由-答案对上进行训练，学生学会更好地将答案预测与理由生成联系起来。（3）通过反事实推理作为辅助训练目标，学生学会不采取推理捷径，而是更尊重理由。（4）尽管更忠实，我们的模型的性能与基准模型相当。（5）消融研究表明，尽管表现更好，较大的学生模型更容易不一致。我们的方法能够稳健地纠正不一致性，无论学生模型的大小如何。（6）有了更忠实的学生，我们可以通过修正其理由来更好地提高其性能，展示了我们的方法在模型改进方面的实用性。 图2：我们用于忠实推理的知识提炼框架概述。（a）教师：一个大型LM提示通过对比解码在培训集中的问题和黄金答案中生成一致的原理。（b）学生：一个小型LM经过微调以生成原理，然后通过反事实推理进行回答。 图3:对比解码，通过偏好只有在考虑答案时才更可信的标记，获得更有依据的黄金答案。 推理链蒸馏我们的目标是：1）从一个大型语言模型中得到一致的理由，即那些能够很好地证明正确答案的理由，并将其作为监督信号；2）训练一个自洽的学生模型进行忠实推理，即根据生成的理由给出相应的答案。我们考虑基于语言的推理任务，其中所需的知识不包含在任务输入中。具体而言，我们专注于开放领域的问答（QA），这是之前工作中最常见的设置：给定一个问题q，要求QA系统预测出正确答案a∗。为了可解释性，我们还要求模型提供一个自由文本的理由r，来证明其预测结果。下面我们将描述一个基本的知识蒸馏框架的概述，如图2所示。然后我们讨论其局限性，并在第3节中提出我们的方法。 2.1 生成理由注释我们不再要求人类为每个问题-答案对{q, a∗}注释理由，而是通过使用上下文学习自动从教师模型中获取理由。这个想法是在提供新实例之前，用一些带注释的示例作为演示来提示一个冻结的语言模型作为教师。每个示例包括从训练集中随机抽样的问题q，正确答案a∗和一个人工注释的理由r，该理由证明了为什么a∗是正确的。提示p的格式如图2所示（左侧的提示部分）。为了获得新问题q的理由，一种基本策略可以是贪婪解码，即在每一步选择最合理的令牌： 2.2 训练学生模型现在有了带注释的训练数据{q, r, a∗}，我们可以训练一个较小的模型作为学生。有许多方法可以实现一个既能进行预测又能生成理由的问答模型。在这项工作中，我们专注于自我理由化的范式，即学生首先生成理由，然后在生成的理由的条件下预测答案。这与其他相关工作不同，其他工作进行后理由化，即在预测答案后生成理由，或者进行多任务学习，将理由生成视为除了答案预测之外的辅助任务。之所以这样选择，是因为后两种范式中的理由生成不会影响决策，因此一开始就无法保证理由的忠实性。给定一个问题q，学生模型被训练输出一个由理由令牌和答案令牌连接而成的序列，如图2所示（右侧的输出部分）。一个直接的实现方法是使用标准语言模型损失对教师使用的生成的银标训练数据进行微调的文本到文本语言模型训练： 我们称之为事实推理损失。 3 蒸馏一个自洽的学生在前面的部分中，描述了基本的知识蒸馏过程，但存在两个重要问题。首先，神经语言模型已知存在幻觉问题，即它们经常生成与输入不相关的文本（Maynez等，2020；Ji等，2022）。这将导致生成的理由不支持给定的答案。理由与答案之间的不一致将传递给学生模型，使学生误以为答案预测与理由生成无关。其次，学生模型可能会通过采取推理捷径（Branco等，2021）学习预测答案，而不考虑生成的理由（即使答案预测是基于理由条件的）。这是因为各种隐式推理任务数据集中存在问题和答案之间的虚假相关性（Gururangan等，2018；Zellers等，2019；Blodgett等，2020）。上述两个问题将导致一个不可信的学生，其生成的理由不能始终合理地证明其答案。为了缓解这个问题，我们提出了以下两种相应的技术。 3.1 始终如一的老师对比解码为了鼓励教师生成更多支持答案的本体理由，我们提出的方法扩展了一种称为对比解码的先前技术，用于生成开放式文本（Li等，2022b）。其核心思想是在解码过程中搜索那些在考虑答案时更可信的理由标记，而不是那些即使没有答案也相当可信的理由标记。 为了实现这一思想，我们首先通过向同一教师提供一个扰动答案a′来模拟幻觉行为，然后在给定答案的情况下获取任何标记ti的可信度增长。我们研究了两种扰动答案的方式：将a′设置为空字符串或者设置为除a∗之外的错误答案。第一种方式（空字符串）对那些在没有考虑黄金答案a∗时通常可信的标记进行惩罚，因为幻觉语言模型不会考虑黄金答案。第二种方式（错误答案）进一步鼓励教师生成更具区分性的理由，以区分黄金答案和错误答案。图3展示了贪婪解码和对比解码对于一个示例问题的生成结果。 为了在语言流畅性和与答案a∗的关联性之间取得平衡，我们将可信度增长通过聚合的方式融入到公式1中，作为我们最终的对比解码策略： 3.2 一个忠实的学生：反事实推理为了鼓励学生在生成的理由中进行忠实的推理，我们训练学生进行反事实推理（Roese，1997），即当理由导致不同的答案时，相应地回答。这有助于消除问题和黄金答案之间的推理捷径（图4），因为现在要求学生对相同的问题给出不同的答案。为了实现这个想法，我们首先用一个错误答案a′（使用与3.1节相同的采样策略）替换方程4中提供给教师的黄金答案，就好像a′是正确的。因此，我们得到了一个导致错误答案a′的反事实理由r′。然后，我们训练模型在将r′直接作为教师强制输入解码器时生成a′（语言建模损失仅应用于答案令牌ti ∈ a′）： 为了避免让学生对任务产生困惑，我们在输入序列到编码器和输出序列到解码器的开头添加关键词[Factual]（或[Counterfactual]）来指示学生训练目标方程2（或方程5）（请参见图4中的示例输入和输出）。总体训练损失是方程2和方程5的总和。 图 4：反事实推理，教导学生忠实地进行推理，即根据理由做出不同的回答。 实验我们的实验旨在回答以下研究问题：（1）我们的对比解码策略是否能够产生更一致的教师模型？（2）更一致的教师模型和反事实推理目标是否能够导致更忠实推理的学生模型？（3）通过修改学生生成的理由，我们是否能够更好地控制自洽学生模型的预测？ 4.1 数据集我们在几个知识密集型的基于语言的推理任务上进行实验：（1）CSQA（Talmor等，2018）是一个包含五个选项的问答数据集，测试关于日常概念的常识。（2）StrategyQA（Geva等，2021）是一个二元（是&#x2F;否）问答数据集，问题中所需的推理步骤是隐含的。（3）CREAK（Onoe等，2021）是一个事实核查（真&#x2F;假）数据集，测试关于实体知识的常识推理。（4）QASC（Khot等，2020）是一个包含八个选项的问答数据集，需要进行知识事实的检索以及使用常识来组成这些事实。 由于这些数据集的测试标签不公开，我们将官方的开发集作为我们的测试集，同时将官方的训练集随机划分为新的训练集和开发集。 4.2 评估指标（1）为了评估教师生成的理由与提供的黄金答案之间的一致性，我们使用LAS指标（Hase等，2020），其核心思想是衡量理由对于模拟器预测黄金答案a∗的帮助程度，计算方式为在提供理由作为输入和不提供理由作为输入时任务性能的差异，即Acc(qr → a∗) - Acc(q → a∗)。（2）为了评估学生生成的理由的忠实性，我们同样使用LAS指标来衡量理由对于模拟器预测学生的预测a′的帮助程度，即Acc(qr → a′) - Acc(q → a′)。我们分别使用经过微调的T5-large模型（Raffel等，2020）实现每个模拟器。（3）为了评估学生在下游数据集上保持任务性能的好坏，我们使用准确率作为指标。 4.3 实现细节我们使用GPT-neox（Black等，2022）作为教师模型，它具有20亿个参数，因为模型检查点是公开可用的，这使得我们可以离线托管它，并按照对比解码所需的标记概率进行访问。然后，我们通过在方程4中使用空字符串或错误答案作为扰动答案a′，分别实现了两种教师变体。然后，我们使用获得的理由微调两个T5-3b语言模型作为学生模型。对于这两种变体，我们使用实际训练损失方程2和反事实训练损失方程5来训练学生模型。 4.4 基线方法思维链（CoT）由于我们从GPT-neox（具有20亿个参数）（Black等，2022）中引出理由来训练学生模型，我们首先使用CoT提示（Wei等，2022b）要求相同的模型（GPT-neox）进行解释，然后进行预测。 从人类学习为了展示我们自动生成理由注释的自动方式的优势，我们将这个基线实现为在人类注释的理由上进行微调的T5-3b语言模型，人类注释的理由获取成本高昂且可能存在噪音。 从贪婪解码学习我们将这个基线实现为在使用与我们的主要方法相同的语言模型进行贪婪解码得到的理由上进行微调的T5-3b语言模型。我们还实现了另一个变体，在微调学生模型时添加了反事实推理损失，其中错误答案的理由是通过贪婪解码获得的。 我们还通过使用只基于实际推理的对比解码得到的空&#x2F;错误答案理由，训练学生模型来实现了我们方法的两个基线方法。 我们使用固定的随机种子运行所有实验，并报告平均结果，每个实验运行5次。 4.5 主要结果对比解码是否能够导致更一致的教师模型？ 图5展示了不同教师生成的理由与黄金答案之间的一致性，以LAS指标进行度量。在四个数据集上，与人工注释和贪婪解码相比，使用空字符串或错误答案进行对比解码可以产生更一致的理由。这证明了我们对比解码策略在鼓励教师生成更与主题相关的理由方面的有效性。此外，与使用空字符串相比，使用错误答案进行对比解码效果更好。这表明通过与错误答案进行对比，教师可以生成更能导致黄金答案的可区分的理由，从而获得更高的一致性。贪婪解码相对于人工注释产生的理由更不一致，验证了我们的观点，即语言模型容易生成与黄金答案不相关的文本。 我们还对StrategyQA数据集生成的100个理由进行了人工评估。 评估者根据三个维度对理由进行评判：1）语法正确性（理由是否语法正确？）2）新信息（理由是否提供了问题中未表达的新信息？）3）支持答案（理由是否证明了答案的正确性？）。表1证实，我们的两种对比解码策略产生的理由比贪婪解码更具信息量且更与主题相关，尽管语法正确性稍差。我们在附录的表2中列举了示例，展示了对比解码产生的理由与黄金答案相比，比贪婪解码更一致。 更一致的教师是否能够训练出更忠实的学生模型？ 图6（每个子图的上部分）展示了在实验数据集上使用LAS指标度量的学生模型的忠实性。首先，与知识蒸馏方法相比，思维链（CoT）方法在四个数据集上通常得到更低的LAS分数，表明生成的理由不能忠实地反映思维链中的决策过程。其次，我们观察到，使用对比解码产生的空字符串或错误答案的理由训练的学生模型通常比基线方法获得更高的LAS分数。结合对教师模型一致性的观察（图5），这验证了更一致的教师能够训练出更忠实的学生模型，并且教师模型在训练数据中产生的不一致性会被学生模型继承。 反事实推理损失能进一步提高忠实性吗？ 图6显示，使用反事实推理训练损失进行额外微调的学生模型比仅使用事实推理训练进行微调的学生模型表现出更高的忠实性。这验证了反事实推理可以进一步提高学生模型的忠实性，因为它可能仍将理由生成和答案预测视为两个独立的过程。 一个忠实的学生模型是否仍能保持其性能？ 图6（每个子图的下部分）显示了学生模型的准确性。首先，与知识蒸馏方法相比，思维链方法的准确性较低，显示了教师模型（理由）和标记数据集（答案）相结合的好处。其次，所有的知识蒸馏方法都获得了类似的性能。结合对忠实性的观察，这表明我们的方法可以提高模型的忠实性而不损害其性能。需要注意的是，从人工注释学习的学生模型相比其他学生模型稍微获得了更好的结果。这是因为人工理由与答案的一致性较低（如图5所示）。因此，学生模型学会更独立地生成理由和预测答案，从而利用虚假相关性并获得更好的性能。我们进一步的分析（§ 4.7）显示，这种性能提升是可疑的，因为改变理由大多数情况下不会显著改变学生模型的预测结果。 4.6 对学生模型大小的剔除我们对学生模型大小进行了剔除实验，以了解其忠实性和性能的影响。从图7中，我们观察到较大的学生模型获得了更高的性能，但忠实性较低。这证实了为推理所需的知识存储需要足够的容量（Wei等人，2022a），但较大的模型在回答问题时也更独立于理由。尽管如此，我们的模型比基准模型更忠实，并且在不同模型大小下的性能相当。 4.7 控制学生模型行为忠实的理由的一个重要作用是，我们可以通过改变其理由来更好地控制学生模型的行为。如果模型能够与其理由一致地进行预测，我们可以通过扰动或改进其理由来削弱或提高其性能。为了验证这一点，我们对学生模型生成的理由进行了两种类型的编辑，即扰动和改进，具体如下所述。然后，我们将编辑后的理由直接馈送给学生模型的解码器（作为教师强制），并观察学生模型是否会相应地行动，即由于较差（或较好）的理由而预测更差（或更准确）。 理由扰动 对于扰动理由，我们随机替换学生模型生成的理由中的50％令牌，然后将扰动后的理由r’馈送回学生模型的解码器。我们最终计算性能下降（或敏感性），即Acc(qr → a∗) - Acc(qr’ → a∗)。图8（下部）显示了在CSQA和CREAK上的结果。首先，扰动经过人工注释微调的学生模型的理由对其性能几乎没有影响（在CSQA上下降到1.1%），这意味着学生模型在进行预测时很大程度上忽略了理由。其次，从对比解码获得的理由（包括空字符串或错误答案）进行学习，使学生模型对理由扰动更加敏感，相比贪婪解码更敏感。这再次验证了训练忠实学生的必要性，即需要一个一致的教师。最后，我们的反事实训练损失进一步提高了学生模型的敏感性，表明学生模型对理由更忠实。 理由改进 作为代理改进，我们通过要求教师使用每种对比解码策略为黄金答案进行理由化，自动获得了理想的理由r∗。对于通过人工注释进行训练的学生模型，我们直接使用注释的理由作为理想的理由。然后，我们计算性能增益，即Acc(qr∗ → a∗) - Acc(qr → a∗)。图8（上部）显示了在CSQA和CREAK上的结果。首先，我们观察到理想的人工注释理由并没有像机器生成的理由那样带来很大的性能增益。这证明即使经过人工注释训练，学生模型仍然容易对其理由不忠实。其次，我们观察到对比解码（包括空字符串或错误答案）导致学生模型的性能增益更大。通过添加反事实训练，性能增益进一步增加。这证明了我们的方法带来的优势，即我们可以通过改进学生模型的理由来更成功地调试推理模型。 5 相关工作自由文本理由已经提出了各种数据集，旨在在每个任务实例旁边收集人工注释的理由（Camburu等人，2018; Rajani等人，2019; Aggarwal等人，2021），旨在训练下游模型以用自然语言解释其预测。然而，人工注释是昂贵的，并且所得到的理由质量被报道为较差（Aggarwal等人，2021; Sun等人，2022）。我们的工作利用提示的语言模型自动获取理由，支持正确和错误答案，只使用少量注释示例作为演示。支持错误答案的理由进一步使学生能够进行反事实推理，这是现有人工注释不具备的。 提示式自我理由化模型最近的研究提出了在进行预测之前，提示大型语言模型生成自由文本理由的方法（Nye等人，2021; Wei等人，2022b）。然而，这种技术依赖于非常大的语言模型（超过1000亿个参数）才能有效工作（Wei等人，2022b，a），这需要大量的计算资源或昂贵的API调用（Shridhar等人，2022）。与此同时，这种模型生成的理由已被证明与上下文相矛盾（Ye和Durrett，2022），并且无法忠实地表示潜在的推理过程（Wang等人，2022）。相比之下，我们的学生模型通过较小的语言模型训练，更忠实地生成其理由。 知识蒸馏存在一些工作探索从大型语言模型中提取理由知识，将其蒸馏到作为学生的小型语言模型中。Chan等人提出了学习仅从带有理由的教师模型中预测答案的学生模型。Eisenstein等人提出了训练学生模型提取包含答案的句子的方法，但这种方法不适用于需要背景知识的推理任务。 施耐德等人提议训练学生模型提出并回答分解主要问题所需的子问题，该方法专门用于解决数学问题（Cobbe等人，2021），并配有一个方程式生成器，以指导学生的学习，而我们没有这样的约束条件。Li等人提出了在生成答案和理由的联合任务上训练学生模型的方法，这仅作为正则化项，不会影响学生模型在推理过程中的预测。更重要的是，Shridhar等人和Li等人都没有考虑理由的忠实性，而这对于检查学生模型的行为至关重要。 6 结论本工作提出了一种忠实的知识蒸馏框架，用于从大型教师模型中学习一个小型、自洽的CoT模型。为了确保学生模型的推理过程忠实，我们提出了（1）对比解码用于获取一致的教师模型和（2）反事实推理用于教授忠实的学生模型。实验证明，与基线相比，这两种技术共同导致了一个更忠实的学生模型，同时保持了较高的性能准确性。我们进一步的分析表明，改变理由对学生模型的行为影响更大，因此通过改进理由可以更成功地调试模型。 局限性 与标准的知识蒸馏过程相比，我们的方法在准备训练数据和训练学生模型时需要额外的计算。首先，我们的对比解码需要在教师模型中进行额外的前向传播，比贪婪解码多一次，以获得每个生成的令牌的扰动可信度（方程式4）。其次，我们的知识蒸馏过程引入了额外的训练数据，用于用反事实推理目标（方程式5）训练学生模型。除了计算成本之外，这项工作侧重于提高理由的忠实性，而不是性能，这与之前利用理由仅提高性能的方法是互补的。 伦理声明我们的知识蒸馏过程利用大型语言模型获得理由注释，这可能暴露出这些模型中编码的社会偏见（Lucy和Bamman，2021）。这种偏见可能会进一步传递给学生模型。然而，我们的方法提高了理由的忠实性，使得学生模型的预测是可靠的。如果没有忠实的理由，用户将不清楚模型是否基于某种意外的偏见进行预测。","tags":["Paper"]},{"title":"Monash IT FIT5037","path":"/2023/07/19/Monash-IT-FIT5037/","content":"FIT5037Week 1: OverviewType of Network Attacks Interruption Availability Interception Confidentiality Modification Integrity Fabrication Authentication Security Goals Secrecy&#x2F; Confidentiality(CONF) Encryption Integrity Message Authentication Code(MAC) Digital Signature Authentication The digital signature, biometrics, password Non-repudiation(NR) The digital signature, biometrics Availability&lt;!–more–&gt; Week2 Cryptography Foundations for Network SecurityBlock cipher: process one input block at a time produce one output block for each input block common block sizes: 64, 128 and 256 bits Stream cipher: process input elements continuously produces one element at a time common element size: 1 bit or 1 byte operation is generally exclusive OR (XOR) between input elements and stream cipher key ECB, CBC,CFB, OFB, CTR RSA is not indistinguishable under chosen-cipher text attack Why are attacks in the IND-CPA model effective against RSA? Encryption can be performed by anyone. RSA encryption is deterministic. Probabilistic PKE AUTHentication + INTegrity: Diffie Hellman Key Exchange Digital Signature and MAC (Message Authentication Code) and HashDigital signatures reproduce the electronic version of the normal signatures proof of identity (authenticity of the message origin) proof of message integrity non-repudiation MAC Provides message integrity and authenticity MAC does not distinguish between two participants Hash One way function collision resistance Provides integrity check MAC Replay Attack and CountermeasureNonce + Challenge Response Protocol Countermeasure: Nonce Every time Bob wants to communicate, Alice sends a nonce (a challenge) to Bob. Then Bob calculates: Hash(key, “I’m Bob” || nonce). Alice receives the above hash value and recalculates it for verification. Since the hash value is different for each nonce, replay attacks are mitigated. Week3A: Public Key Infrastructure Authentication Confidentiality Integrity Non-repudiation Trust Digital SignaturesEncrypt message with private key -&gt; Generate digital signature Decrypt digital signature with public key -&gt; Verify digital signature -&gt; Check if M &#x3D;&#x3D; M’ How PKI Defeats the MITM AttackAlice needs to obtain a certificate from a trusted entity (Certificate Authority - CA). After verifying Alice’s identity, the trusted entity issues a certificate containing Alice’s name and publickey. Alice sends the entire certificate to Bob. Bob uses Alice’s public key to verify the certificate - ensuring it is from Alice and not someone else. The certificate cannot be forged or tampered with. Core Functions of a CASubject Verification Ensure that the entity applying for a certificate possesses or represents the identity in the subject field. Signing Digital Certificates The CA uses its private key to generate two digital signatures for the certificate. Once the signatures are applied, the certificate cannot be modified. Anyone with the CA’s public key can verify the signatures. Get a Certificate from CA:Step 1: Generate a public&#x2F;private key pair Step 2: Generate a certificate signing request (CSR); Attack Scenario: Genuine CertificateThe attacker forwards a genuine certificate to Alice. Alice recognizes the certificate as genuine, so she encrypts secret with the certificate’s public key andsends it to the “server.” The attacker intercepts the request but cannot decrypt the secret because they do not possess the privatekey. Attack Scenario: Fake CertificateThe attacker forges a certificate for the domain example.com using their own public key. The CA does not sign the certificate as the attacker is not associated with example.com. The attacker attempts to self-sign the certificate and sends it to Alice. Alice’s browser raises a warning as it cannot find any trusted certificate to validate the receivedcertificate. Attack Scenario: Attacker’s CertificateThe attacker possesses a valid certificate of their own. The attacker sends their certificate to Alice. Alice’s browser checks if the subject field of the certificate matches Alice’s intent. The received certificate’s validity is checked. The certificate’s subject (common name) is verified to match the server’s hostname. Attacks Against PKIMan-in-the-Middle Proxy The proxy creates a self-signed CA certificate installed on the user’s browser. The proxy intercepts communications. Attacks on CA Verification Process Attack on CA Signing Process: Private key compromise. How to protect private keys: Use a hardware security model. Attacks on Algorithms: Digital certificates rely on one-way hash and digital signatures. Use stronger algorithms. User Confirmation Attacks: Certain software does not compare the information within the certificate (commonname field) with the user-provided or approved information: Security vulnerability in Common Name. Week 3B: Email Security What are the main services provided by PGP? Key management, confidentiality, integrity, authenticity. What is the purpose of detached signatures? Signatures are sent separately. Two files: the original data and the signature. Why is R64 encoding used for email applications? Some email servers do not accept binary emails and require printable characters. R64 uses ASCIIcharacters, all of which are printable. What is S&#x2F;MIME? What encryption functions does S&#x2F;MIME use? S&#x2F;MIME &#x3D; Secure MIME 3DES&#x2F;AES for encryption, Elgamal for key exchange, SHA for signing What is DKIM? How does DKIM email authentication service differ from S&#x2F;MIME or PGP? The sender’s email server signs the email. DKIM is used for email server to email server communication, while PGP and S&#x2F;MIME are end-to-endencryption. Why is Transport Layer Security (TLS) not sufficient to protect email? Answer: TLS protects communication between two hops; all intermediate hops see plaintext. Simple Mail Transfer Protocol (SMTP) Text-only Limited to ASCII characters Size limitations Command-only Issues No security - plaintext messages PGP (Pretty Good Privacy) Key management Authentication (AUTH) + Integrity (INT) Digital signatures (No-Repudiation) Confidentiality (CONF) Encryption: Symmetric encryption IDEA, 3DES-EDE, AES Public-key encryption RSA, Elgamal, Digital Signature Algorithm Hash MD5, SHA-1, SHA-2, SHA-512 Encryption steps: HASH(M) &#x3D;&gt; digest RSA(digest, private key) &#x3D;&gt; digital signature sig RSA is a public-key encryption method used for signing. digital signature sig || M &#x3D;&gt; certificate Compress(certificate) &#x3D;&gt; compressed certificate AES(session key, compressed certificate) &#x3D;&gt; encrypted compressed certificate (using symmetricencryption) RSA(session key, recipient’s public key) &#x3D;&gt; encrypt session key for non-peer-to-peer encryption of thesession key Another user can retrieve the session key of 128 bits using their private key Sent to the recipient: encrypted compressed certificate || encrypted session key Decryption steps: Retrieve the decryption session key using the private key &#x3D;&gt; session key Decrypt the compressed certificate using the session key &#x3D;&gt; compressed certificate Obtain M || digital signature by decompressing the data &#x3D;&gt; M || digital signature &#x3D;DeCompress(compressed certificate) Verify Hash(M) and Hash(M’) Hash(M’) &#x3D; Decrypt the digital signature using the sender’s public key. Compression: PGP uses ZIP compression algorithm after applying the signature and before encryption. Why is this order important? Signing: It is best to sign the original message before compression to ensure the signature is based onthe original message. Transmission: Compression saves on email transmission and file storage space. Encryption: Encrypting after compression can enhance encryption because compression reduces redundancy inthe message. Radix-64 Encoding Used for compatibility with email protocols Text &#x3D;&gt; Binary &#x3D;&gt; ASCII characters (every 6 bits &#x3D; 1 ASCII character) Key Management Send Key Identifiers (KeyID) instead of the full public key for bandwidth efficiency. Users can have multiple key pairs. Two types of keyrings need to be maintained: User’s own public&#x2F;private key pairs Store the encrypted private key instead of the plaintext private key. Public keys of other communication partners Store the public keys of other users indexed by their KeyID. Trust Model Does not rely on Certificate Authorities (CAs). Each user is their own CA. Users sign the keys of users they trust, forming a web of trust. If there is a chain of signatures leading to a trusted key, the trusted key is signed by other trusted keys. Restrictions Misuse Leakage No forward secrecy Public key exchange is required. Targeted attacks against PGP KeyID. May hinder useful functionalities such as search, spam filtering, and subject extraction. Lack of non-repudiation and authentication in compromised scenarios. What is S&#x2F;MIME? What encryption functions are used in S&#x2F;MIME? S&#x2F;MIME stands for Secure&#x2F;Multipurpose Internet Mail Extensions. Digital Signatures RSA Hash Functions MD5, SHA-1, SHA-2 Session Key Encryption Key Transport: RSA, ElGamal (asymmetric) Key Agreement: Diffie-Hellman (DH) Message Encryption (Symmetric) AES-128 Triple DES (T3-DES) MAC (Message Authentication Code) HMAC with SHA-1 Cipher Modes AES-GCM (Galois Counter Mode) AES-CCM (Counter with CBC-MAC) Multipurpose Internet Mail Extensions (MIME) MIME supports different types of content. X.509 v3 certificates Each client has a trusted CA certificate list. Own key pairs and certificates signed by trusted CAs. DomainKeys Identified Mail (DKIM) Outgoing emails from the sender’s domain must be signed using the sender domain’s secret key before leavingthe domain. Transparent to the user Mail Submission Agent (MSA) signs Mail Delivery Agent (MDA) verifies The recipient can verify the signature using the domain’s public key. The public key is stored in DNS servers, and the signature is attached. What is DKIM? How does DKIM email authentication service differ from S&#x2F;MIME or PGP? DKIM stands for DomainKeys Identified Mail. The sender’s email server signs the email. DKIM is used for authentication between email servers, whereas PGP and S&#x2F;MIME provide end-to-end encryption. Q: Why don’t email service providers deploy end-to-end (user-to-user) email encryption?Encryption prohibits email systems from providing useful functionalities such as filtering, topic extraction,targeting ads, and search. Week4：IPSecVirtual Private Network (VPN) Isolation Tunneling Multiple logical overlay networks coexist on the same physical network, each network providing its ownprivate services. AnswerPlacing the VPN server in front of the firewall is better. VPN also has the functionality of a firewall. Afterthe VPN gateway, the messages will be decrypted, allowing the firewall to inspect the details of the packets. IPsec: Internet Protocol SecurityIPSec is a security protocol used to protect network layer data. It provides security processing at theInternet layer for IP datagrams, protecting them according to the security policies of the communicating IPnodes before forwarding them to the network interface layer. The receiving IP node verifies the datagrams based on established security parameters and rejects those thatare not protected according to the defined policies. The services provided by IPSec include: Authentication, Integrity, Confidentiality, No-Repudiation (butdoes not hide identity). Authentication: Verifies the claimed identity of the data source, ensuring the authenticity of the sender. Integrity: Data Integrity Anti-Replay: It can detect tampering with and duplicate arrival of individual IP datagrams. Confidentiality: Protects data from unauthorized disclosure and provides limited trafficconfidentiality. It hides the source IP address, destination IP address, size of IP datagrams, andcommunication frequency. The entire process of IPsec consists of five steps: Initiation: Something needs to trigger the creation of our tunnel. For example, when we configure IPsec on arouter, we use an access list to tell the router which data to protect. When the router receives content thatmatches the access list, it initiates the IKE process. The tunnel can also be initiated manually. IKE Phase 1: We negotiate a security association to build the IKE Phase 1 tunnel (ISAKMP tunnel). IKE Phase 2: In the IKE Phase 1 tunnel, we build the IKE Phase 2 tunnel (IPsec tunnel). Data Transfer: We send user data through the IKE Phase 2 tunnel to protect the user data. Termination: The IPsec tunnel will terminate after a period of time when there is no user data to beprotected. IPSec has two main protocols: Authentication Header (AH): Integrity Data-Origin Authentication Anti-Replay Access Control No Confidentiality Encapsulating Security Payload (ESP): Integrity Data-Origin Authentication Anti-Replay Confidentiality ESP can provide only confidentiality (CONF), only authentication (AUTH), or both. IPSec has two modes: Encapsulation Mode Transport Mode: Inserts the IPsec header into the IP packet to protect the data. Host-to-host Tunnel Mode: Keeps the original packet and adds a new header. The ESP&#x2F;AH header follows the new IP header, while theoriginal IP packet remains unchanged. Gateway-to-gateway Anti-replay ServicesReplay: AUTH packet Case If the received sequence number is smaller than the size of the current window’s leftmost edge, discard thepacket. If the received sequence number is within the range of the current window, check if the packet has beenreceived. If the received sequence number is greater than the maximum size of the current window, set the receivedsequence number as the maximum edge and calculate the minimum edge using N-W+1. If the recipient sets N&#x3D;60 and window size W&#x3D;64, the range should be 0-60. The IPSec policy file contains a list of entries:3 IPSec Policies: DISCARD Discard the packet. PROTECT Protect the packet using AH and ESP security protocols. BYPASS Bypass IPSec processing. Security Association (SA)SA is a contract between two entities that specifies how they will communicate using security services. Security Association Database (SAD) AH information ESP information SA lifecycle IPSec protocol mode (Transport&#x2F;Tunnel) IPSec Architecture Authentication Header (AH): Extended header for message authentication. Encapsulating Security Payload (ESP): Provides encryption with combined confidentiality and messageintegrity. Internet Key Exchange (IKE): Key management scheme for IPSec. SA establishment: Participants of the IKE protocol first establish security associations to define thesecurity parameters for the IPSec session, such as encryption algorithms, authentication algorithms, keylengths, etc. These parameters will be used for encrypting and authenticating IP packets. Key exchange: IKE uses the Diffie-Hellman key exchange algorithm to negotiate and generate shared keys.Through Diffie-Hellman key exchange, communicating parties can securely generate shared keys without directlytransmitting the keys. Authentication: The IKE protocol supports various authentication methods such as pre-shared keys,digital certificates, Public Key Infrastructure (PKI), etc. Authentication ensures that communicating partiescan mutually verify each other’s identities and prevent man-in-the-middle attacks. Key negotiation and establishment: Through the negotiation process, IKE parties use the Diffie-Hellmankey exchange algorithm to generate shared keys and use these keys to establish various keys required for theIPsec session, such as symmetric keys and asymmetric keys. Anonymising NetworkC. IPsec does not provide anonymity on the Internet. It is primarily used for encrypting and securing thetransmission of data, rather than hiding or anonymizing the identities of senders or receivers. Solution: decentralized, anonymous network A. When using Encapsulating Security Payload (ESP), it provides confidentiality. ESP is part of the IPsecprotocol suite and is used to provide encryption and authentication of data over IP networks. B. IPsec provides anti-replay properties, meaning it can prevent attackers from replaying captured packets,thus protecting the integrity and security of the communication. D. When using Authentication Header (AH), IPsec provides integrity. AH is also part of the IPsec protocolsuite, and it provides integrity and authentication of data but does not provide encryption. No CONF TOR Distributed Anonymous Communication ServiceThe sender must negotiate an encryption key with each router. Components of Tor Client Server Tor (Onion) router: Special proxy relays application data. Directory server: Provides Tor router information. Process The last router sees the plaintext data but doesn’t know where the message originated from. Each router only knows its predecessor and successor. Process of a client sending a message: The client retrieves the Tor node list from the directory server. The client randomly selects a path to the destination server. The client negotiates an AES key with each router (each router has its own encryption key). The client encrypts the message: C3 &#x3D; Encrypt(K3, data||IP_Server) C2 &#x3D; Encrypt(K2, C3 || IP_OR3) C1 &#x3D; Encrypt(K1, C2 || IP_OR2) The client sends an IP packet: IP_Client || IP_OR1 || C1 Packet processing at each router: Packet arrives at OR1: C2 || IP_OR2 &#x3D; DEC(K1, C1) Cache IP_Client, IP_OR2 Send an IP packet: IP_OR1 || IP_OR2 || C2 Packet arrives at OR2: C3 || IP_OR3 &#x3D; DEC(K2, C2) Cache IP_OR1 || IP_OR3 Send an IP packet: IP_OR2 || IP_OR3 || C3 Packet arrives at OR3: IP_Server || data &#x3D; DEC(K3, C3) Cache IP_OR2, IP_Server Send an IP packet: IP_OR3 || IP_SERVER || data Week5A：FirewallRequirements of a firewall All the traffic between trust zones should pass through the firewall. Only authorized traffic, as defined by the security policy, should be allowed to pass through. The firewall itself must be immune to penetration, which implies using a hardened system with securedOperating Systems. Determines the direction in which requests may be initiated and are allowed to flow through the firewall. Ittells whether the traffic is “inbound” (From the network to the firewall) or vice-versa “outbound” Firewall Policy• User control: Controls access to the data based on the role of the user who is attempting to access it. Applied to usersinside the firewall perimeter.• Service control: Controls access by the type of service offered by the host. Applied on the basis of network address, theprotocol of connection and port numbers.• Direction control: Determines the direction in which requests may be initiated and are allowed to flow through the firewall. Ittells whether the traffic is “inbound” (From the network to firewall) or vice-versa “outbound” Firewall actions• Accepted: Allowed to enter the connected network&#x2F;host through the firewall.• Denied: Not permitted to enter the other side of firewall.• Rejected: Similar to “Denied”, but tells the source about this decision through ICMP packet. three types of firewalls:• Packet Filter Firewall• Stateful Firewall• Application&#x2F;Proxy Firewall Overview of TLSTransport Layer Security (TLS) provides a secure channel between two communicating applications. Integrity: Channel can detect any changes made to the data during transmission Authentication: At least one end of the channel needs to be authenticated, so the other end knows who itis talking to. Confidentiality: Nobody other than the two ends of the channel can see the actual content of the datatransmitted. TLS Layer • TLS sits between the Transport &amp; Application layer• Unprotected data is given to TLS by the Application layer• TLS handles encryption, decryption, and integrity check• TLS gives protected data to the Transport layer TLS HandshakeBefore a client and server can communicate securely, several things need to be set up first: Encryption algorithm and key MAC algorithm Algorithm for key exchange Data is transferred using records:fragmentation: ≤ 16KB Each record contains a header and a payload. Padding Oracle Attack The goal of a padding oracle attack is to compromise the confidentiality of encrypted data, especially in caseswhere block cipher modes (such as CBC mode) are used for encryption. In this mode, the plaintext data is divided into fixed-sized blocks and transmitted after encryption using akey. Each encrypted block is encrypted using the ciphertext of the previous block as the initialization vector(IV). The steps of a padding oracle attack are as follows: The attacker intercepts the encrypted ciphertext and selects a specific encrypted block to attack. The attacker modifies the last byte of that encrypted block and sends the modified ciphertext back to thedecrypting entity. When the decrypting entity attempts to decrypt the modified ciphertext, if the padding verification stepfails (for example, if the format of the padding bytes is incorrect), the decrypting entity will return anerror, indicating a padding verification failure. The attacker leverages the result of the padding verification failure to infer the true value of the lastbyte of the modified ciphertext block. This is because the padding verification step provides information aboutthe correctness of the padding bytes’ format. By continuously modifying the padding bytes until the paddingverification succeeds, the attacker can deduce the correct value of the padding bytes. By repeating the above steps byte by byte, the attacker can deduce the value of each byte in the encryptedblock in sequence. Countermeasures Remove server responses Use AES GCM model - authenticated encryption Ensure that the ciphertext is not modified during the transmission Week 5:Wireless NetworkWireless Network Security Overview and Countermeasures Wireless networks face various security threats, but there are countermeasures to mitigate these risks. The basic elements of the IEEE 802.11 wireless security standard include WEP (insecure), WPA, and WPA2. Understanding the vulnerabilities in the implementation of WPA2 and analyzing the unique threats posed byphysical layer interference attacks. Wireless Security: The Problem Channel: Broadcast nature makes it easier for eavesdropping and interference. Mobility and Accessibility: Mobile devices are more susceptible to threats. Resources: Limited memory&#x2F;power makes it challenging to implement robust security systems. The MAC address (Media Access Control Address) is a unique identifier for network interfaces used to identifydevices on a local network. However, MAC address-based authentication can be bypassed because MAC addresses canbe spoofed or forged. Attackers can use specific software or tools to modify their device’s MAC address, masquerading as a trusteddevice. This deception allows attackers to bypass MAC address-based authentication measures and gainunauthorized network access. Therefore, although the MAC address serves as a unique identifier for a device, it is not a reliableauthentication mechanism as it can be tampered with or forged. In wireless networks, higher-levelauthentication methods such as WPA&#x2F;WPA2 passwords, enterprise-level 802.1X authentication, etc., are typicallyrequired for stronger security. Threat： Man-in-the-Middle attack Identity theft, media access control(MAC) address spoofing Ad hoc networks Denial of Service(DoS) Network injection Wireless Security Measures Securing transmissions: signal-hiding: to protect the location of access points encryption Securing access points authentication JammingThe adversary interferes with the reception of messages by transmitting a continuous jamming signal, or severalshort jamming pulses. cause DoS Detection: monitor signal strength monitor carrier sensing time Week6：Network AttacksSecurity of WEP in IEEE 802.11b: Encryption Algorithm: WEP uses RC4 to encrypt data. RC4 is a stream cipher algorithm used to generateciphertext by XORing plaintext with a key. Key Management: WEP uses a fixed key for encryption, typically a 40-bit or 104-bit key. The key needs to beshared between the wireless access point and wireless clients. Insecurity Improved IV Management: Detection of IV reuse IV collisions WEP Recommendations: Changing IV for each data packet Some weak implementations of WEP Message modification without knowledge of the key Insecurity INT CRC32 is not a MAC: It is linear CRC32 used with stream cipher: Stream ciphers are linear Modification of ciphertext possible without disclosing m Insecurity of WEP in IEEE 802.11b: Key Weakness: WEP uses a relatively small key space of 40 bits or 104 bits, making it vulnerable toexhaustive attacks. Attackers can attempt all possible key combinations to break WEP encryption. Initialization Vector (IV) Reuse: WEP uses a 24-bit IV to increase encryption strength. However, WEP repeatsthe same IV in practical use, leading to repetitiveness in encrypted traffic, allowing attackers to retrievethe encryption key. Weaknesses in the RC4 Algorithm: The RC4 algorithm used by WEP has certain weaknesses that enable attackersto recover the key by analyzing the encrypted data stream. This type of attack is known as WEP key recoveryattack. Data Integrity Issues: WEP does not provide a mechanism for data integrity checks, which means attackers cantamper with the data without being detected. Key Management and Authentication Issues: WEP employs weak key management and authentication mechanisms,making it susceptible to dictionary attacks and forged authentication attacks. Security of WPA TKIP in IEEE 802.11gTemporal Key Integrity Protocol (TKIP) Key Mixing: IV and key are mixed before RC4 encryption for each packet. Protection against replay attacks using sequence counter. Message Integrity Check (MIC) instead of CRC32 for data integrity. Rekeying: Unique key for each packet. Security of WPA2 CCMP in IEEE 802.11i Authentication: Verifying the identity of users or devices. Confidentiality: Protecting data from being intercepted during transmission by encrypting it. Integrity: Ensuring that data is not tampered with during transmission using checksums and MessageAuthentication Codes (MAC). Difference between TKIP and CCMP: TKIP was introduced to enhance the security of WEP (Wired Equivalent Privacy). It provides encryption,integrity, and message authentication but is relatively weaker. CCMP is the encryption protocol used in WPA2, providing authentication, integrity, and confidentiality. It ismore secure than TKIP. WPA2 (PSK): Password Cracking Attack The attacker starts by monitoring the wireless network (using tools like Wireshark). The attacker identifies connected users on the WiFi (can be discovered using Wireshark). The attacker forges a de-auth packet and forces the victim user to disconnect from the WiFi. When the user reconnects to the WiFi, the attacker captures the handshake containing the PSK hash value. The attacker offline cracks the hash and recovers the password. WPA2: Security Best Practices Avoid WPA2 PSK. Use EAP-TLS certificate-based authentication. Implement rogue access point detection. Use the latest operating system. Network Layer: Lower Layers VPN (Virtual Private Network) and IPSEC (Internet Protocol Security): VPN and IPSEC are excellent when you want to encrypt all network traffic. Transport Layer: TLS (Transport Layer Security): Services provided: Confidentiality: Protects data from being intercepted during transmission through encryption. Encryption: Data is encrypted using cryptographic algorithms, ensuring only authorized parties can decrypt it. - Integrity: Ensures data is not tampered with during transmission through checksums and MessageAuthentication Codes (MAC). - Message Authentication Code (MAC): Used to verify the integrity and authenticity of messages. - Authentication: Verifies the identity of communicating parties through methods like digitalcertificates. Application Layer: SYN Flood Attack: SYN flood attacks exploit vulnerabilities in the TCP three-way handshake process. Attackers continuouslysend a large number of forged SYN packets to servers, depleting the server’s Transmission Control Block (TCB)queue resources. Countermeasure: SYN Cookie Upon receiving a SYN packet, the server calculates a keyed hash using a key derived from the SYN packet. The hash value (H), known as the SYN cookie, is sent to the client (IP address) as the initial sequence number. - The server does not store half-open connections in its queue. - Since the attacker receives a fake IP address, they won’t receive H. - Legitimate clients send H+1 to the server, which the server verifies.2. TCP RST Attack: - TCP RST attacks involve immediately terminating a connection between two hosts by sending RST packets. - TLS cannot prevent TCP RST attacks since TLS operates above TCP and cannot protect the TCP header,allowing the RST flag to be set. - Solution: Use Intrusion Detection Systems (IDS) to detect and prevent TCP RST attacks.3. TCP Session Hijacking Attack: - Hackers can execute TCP session hijacking attacks by capturing packets and manipulating the next sequencenumber and acknowledgement number. - Solution: - Make it difficult to spoof packets by randomizing initial sequence numbers and source port numbers. - Encrypt the TCP payload. DNS Protocol: Authoritative Name Server (Domain Name System): Each DNS zone has at least one authoritative name server responsible for publishing information aboutthat zone and providing authoritative and deterministic responses to DNS queries. Local DNS Cache Poisoning Attack: Attack against user machines: Attackers send forged DNS responses with malicious IP addresses to usermachines that send DNS queries. Attack against local DNS servers (cache poisoning attack): Attackers send forged responses to local DNSservers while they perform iterative queries to DNS servers on the Internet, as long as they arrive before theactual responses. Remote DNS Cache Poisoning Attack: Remote DNS cache poisoning attack requires guessing port numbers, transaction IDs, etc. Cache impact: If no attempts fail, the actual response will be cached by the local DNS server, and theattacker needs to wait for cache timeout to make the next attempt. Countermeasures: DNSSEC (DNS Security Extensions): DNSSEC provides authentication and integrity checks for DNS data but does not provide confidentialityprotection. All answers from DNSSEC-protected zones are digitally signed. TLS&#x2F;SSL: Client and server negotiate encryption algorithms and session keys. Relies on trust and decisions regarding certificate authorities for key issuance. HTTPS is built on top of TLS&#x2F;SSL, defeating DNS cache poisoning attacks. Countermeasure for Kaminsky DNS Cache Poisoning Attack:b. Use DNSSEC c. Randomize source port numbers Week7A：IDSIntruders: Impersonator: Typically refers to someone who attacks a system from the outside. Insider: Typically refers to someone who attacks a system from within. Covert Operator: Can be either an external or internal attacker. Design Objectives: Detect various types of intrusion behavior. Including known and unknown attacks. Adapt to new attacks or changes in behavior. Real-time detection of intrusion behavior. Efficient analysis of user activity. Timely reporting of suspicious incidents. Ensure accuracy. Minimize false positives and false negatives. IDS Models: Signature-based: Unusual behavior is known. Alerts are generated when activity matches a signature. Anomaly-based: Normal behavior is known. Alerts are generated when activity deviates from normal behavior. Requires more time and processing power. Can produce false positives (normal activity identified as malicious) and false negatives (malicious activityconsidered normal). Threshold Anomaly Detection Statistical Anomaly Detection Heuristic-based: Uses machine learning models of normal behavior. Alerts are generated when the model identifies activity as anomalous. IDS Architecture: Auditor: Records all security-related activities for analysis. Analyzer: Analyzes data from the auditor automatically and updates settings as needed. Notifier: Reports detected anomalies and updates settings as needed, triggers appropriate countermeasures. IDS Types: Host-based IDS (HIDS): Deployed on individual systems to detect malicious activity on a single device. Can also be used in adistributed system within a network. Pros: Lower cost, as most HIDS are software-based; visibility into low-level activities; lower falsepositive rates for local threats. Cons: Limited view of the network; potential for system tampering by malicious insiders. Network-based IDS (NIDS): Monitors network traffic to detect intrusion behavior across the entire network. Pros: Can see attacks in network traffic, particularly at the transport or IP layer; difficult to tamperwith. Cons: May require dedicated hardware; challenging to inspect encrypted traffic. Week7B：Denial of service attackGoals Target on availability take out a large site with little computing work How: amplification DoS bug -&gt; design flaw DoS flood -&gt; command bot-net to generate a flood of requests Will bypass SYN flood protection proxy Mitigation Client puzzles given challenges for TCP connection floods, the first data must contain a puzzle solution, otherwise, the TCP connectionis closed. for SSL handshake DoS Challenge C based on TLS session ID Server: check puzzle solution before RSA decryption Limitations Requires changes to both clients and servers Hurts low-power legitimate clients during an attack CAPTCHAs Applies to application layer DDoS Source identification Ingress filtering identify packet source block the attack at the source ISP only forwards packets with legitimate source IP Source identification Traceback DDoS involves many packets on thes ame path Store one link in each packet Path can be long Countermeasures Prevent initial hack Use of firewalls Check ingress&#x2F;egress packets Use a server farm and load balancer to offset the effects of a DDoS attack Change the IP address of the attacked system Week8：Penetration Testing in PracticeTypes of tests: **Application penetration testing (**typically web applications), which finds technical vulnerabilities Infrastructure penetration testing, which examines servers, firewalls and other hardware for securityvulnerabilities. TIPSMAC Only Authenticity Integrity IEEE 802.11i Security of WPA2 CCMP Authentication: Verifying the identity of users or devices. Confidentiality: Protecting data from being intercepted during transmission by encrypting it. Integrity: Ensuring that data is not tampered with during transmission using checksums and MessageAuthentication Codes (MAC). Transport Layer Security (TLS): Confidentiality: Protecting data from being intercepted during transmission by encrypting it. Integrity: Ensuring that data is not tampered with during transmission using checksums and MessageAuthentication Codes (MAC). Authentication: Verifying the identity of communication parties through methods like digital certificates.","tags":["Monash"]},{"title":"Vue自定义组件实现无限滚加载数据","path":"/2021/04/24/Vue自定义组件实现无限滚加载数据/","content":"先上效果图 起因我们的项目是类似于知乎的论坛网站，我们在需求分析设计的时候认为分页用无限滚动的方式加载可以更方便用户消费我们的信息流。 element UI那么我们项目用的是element UI，最开始我们想到用elementUI中自带的无限滚动的组件 基础用法在要实现滚动加载的列表上上添加v-infinite-scroll，并赋值相应的加载方法，可实现滚动到底部时自动执行加载方法。 123456789101112131415161718192021&lt;template&gt; &lt;ul class=&quot;infinite-list&quot; v-infinite-scroll=&quot;load&quot; style=&quot;overflow:auto&quot;&gt; &lt;li v-for=&quot;i in count&quot; class=&quot;infinite-list-item&quot;&gt;&#123;&#123; i &#125;&#125;&lt;/li&gt; &lt;/ul&gt;&lt;/template&gt;&lt;script&gt; export default &#123; data () &#123; return &#123; count: 0 &#125; &#125;, methods: &#123; load () &#123; this.count += 2 &#125; &#125; &#125;&lt;/script&gt; 但是这几个无限滚动只能用于列表&lt;ul&gt;&lt;li&gt; 而在自定义的组件就不起作用 例如我的项目中用的就是自己定义的组件&lt;Article&gt; vue-infinite-scroll之后也上网查了很多Infinite Scroll 的实现方法，比如说引用vue-infinite-scroll 插件 官网 具体可以详见这一篇一个超详细vue无限滚动vue-infinite-scroll插件的配置及使用详解 👆但是这个需要引入插件，而且只适合vue2 滚动事件监听页面那么有没有不需要引入插件的方法呢？ 那么可以在vue中写滚动事件监听页面window.addEventListener (&quot;scroll&quot;, this.load); 之后再用vue的destroyed钩子函数来销毁这个事件监听（一定要销毁） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;script&gt;//检测屏幕滑动高度的 用于无限下拉import &#123; getScrollHeight, getScrollTop, getWindowHeight &#125; from &quot;@/utils/screen&quot;;//获取数据的apiimport &#123; getListInCommend &#125; from &quot;@/api/postlist&quot;;//引入自定义的组件import articleList from &quot;@/components/articleframe/ArticleBody&quot;;export default &#123; name: &quot;TopicList&quot;, components: &#123;articleList&#125;, data() &#123; return &#123; articleList: [], //记录页面信息 page: &#123; current: 0, //当前页面 totalpage:1,//总的页面数量 total: 0, //后台总的文章数 &#125;, &#125;; &#125;, created() &#123; this.init() &#125;, methods: &#123; //加载帖子列表 init() &#123; ... &#125;) &#125;, //拉取数据 load() &#123; let vm = this; if (getScrollTop() + getWindowHeight() &gt;= getScrollHeight()) &#123; if (vm.page.current &lt; vm.page.totalpage) &#123; //先判断下一页是否有数据 vm.page.current += 1; //查询条件的页码+1 this.init(this.activeName); //拉取数据 &#125; else &#123; //没有下一页了喔 &#125; &#125; &#125;, &#125;, //主要是滚动事件监听页面 mounted() &#123; window.addEventListener(&quot;scroll&quot;, this.load); &#125;, //destroyed钩子函数来销毁这个事件监听 destroyed() &#123; window.removeEventListener(&quot;scroll&quot;, this.load, false); &#125;,&#125;;&lt;/script&gt; 总结这算是我在实践中遇到的比较难解决的问题，用了很多种方法都不能实现无限滚动，但是功夫不负有心人，发现了相对简单而且有效的方法 参考：vue页面滚动监听","tags":["Vue"]}]